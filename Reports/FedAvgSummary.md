# Federated Averaging Summary

## Область исследования и мотивация

Данная статья рассматривает подход к машинному обучению, который позволяет тренировать алгоритмы на децентрализованных наборах данных. Это стало возможно благодаря тому, что всё чаще телефоны и планшеты становятся основными вычислительными устройствами для многих людей. Постоянное пользование гаджетами дает доступ к огромным объемам данных, большая часть которых строго конфиденциальна. Модели, обученные на таких данных, обещают значительно улучшить удобство использования, обеспечивая более интеллектуальные приложения.

## Научная и практическая новизна работы

В этой статье делается акцент на non-IID и несбалансированности данных, а также на важности ограничений связи. Также рассматривается алгоритм FederatedAveraging совместно с различными архитектурами (CNN, LSTM).

## Методология решения

* **FedSGD (базовый)**: каждый раунд клиенты вычисляют по одному градиентному шагу и отправляют градиенты на сервер для усреднения.

* **FedAvg**: каждый выбранный клиент за раунд выполняет E локальных эпох SGD на батчах размера B, затем отправляет обновлённые параметры; сервер усредняет параметры с учётом объёма данных каждого клиента.

* Ключевые гиперпараметры:
    * C — доля клиентов за раунд,
    * E — число локальных эпох,
    * B — размер локального батча.

## Экспериментальная проверка решения

| Датасет и модель     | FedSGD, раунды | FedAvg, раунды | Ускорение | Итоговая точность |
|----------------------|----------------|----------------|-----------|-------------------|
| MNIST 2NN (IID)      | 1455           | 32             | 45×       | 97%               |
| MNIST CNN (IID)      | 387            | 18             | 21×       | 99%               |
| Shakespeare LSTM     | 2488           | 41             | 60×       | 54%               |
| CIFAR‑10 CNN (85%)   | 99 000         | 2 000          | 50×       | 85%               |
| Крупная LSTM-модель  | 820            | 35             | 23×       | 10.5%             |
