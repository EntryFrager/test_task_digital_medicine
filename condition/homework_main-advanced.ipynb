{"cells":[{"cell_type":"markdown","metadata":{"id":"VbCeb7l9uxQL"},"source":["# Homework: Not So Basic Artificial Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"JgNPh-JNuxQP"},"source":["Your task is to implement a simple framework for convolutional neural networks training. While convolutional neural networks is a subject of lecture 3, we expect that there are a lot of students who are familiar with the topic.\n","\n","In order to successfully pass this homework, you will have to:\n","\n","- Implement all the blocks in `homework_modules.ipynb` (esp `Conv2d` and `MaxPool2d` layers). Good implementation should pass all the tests in `homework_test_modules.ipynb`.\n","- Settle with a bit of math in `homework_differentiation.ipynb`\n","- Train a CNN that has at least one `Conv2d` layer, `MaxPool2d` layer and `BatchNormalization` layer and achieves at least 97% accuracy on MNIST test set.\n","\n","Feel free to use `homework_main-basic.ipynb` for debugging or as source of code snippets."]},{"cell_type":"markdown","metadata":{"id":"GAYuXM-IuxQQ"},"source":["Note, that this homework requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n","- This notebook with cnn trained\n","- `homework_modules.ipynb`\n","- `homework_differentiation.ipynb`"]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"IbXpYWUn9mKu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wu_wS1DFuxQR"},"outputs":[],"source":["%matplotlib inline\n","from time import time, sleep\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import gzip"]},{"cell_type":"code","source":["# Import your google drive with notebooks\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"bLq4r9uFE2xw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move to folder with homework (all files need to be in one folder)\n","%cd '/content/drive/MyDrive/smthg/path_to_folder/'"],"metadata":{"id":"xXy_vxgg9aWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQVo7WJYuxQS"},"outputs":[],"source":["# (re-)load layers\n","%run homework_modules.ipynb"]},{"cell_type":"markdown","source":["# Digit Classification"],"metadata":{"id":"d6VFN6p2-JFp"}},{"cell_type":"markdown","source":["## Upload MNIST dataset"],"metadata":{"id":"8_pYyYpx9n0i"}},{"cell_type":"code","source":["def load_image(filename):\n","    # Read the inputs in Yann LeCun's binary format.\n","    with gzip.open(filename, 'rb') as f:\n","        data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    # The inputs are vectors now, we reshape them to monochrome 2D images\n","    data = data.reshape(-1, 28, 28)\n","    # The inputs come as bytes, we convert them to float32 in range [0,1].\n","    return (data / np.float32(256)).squeeze()\n","\n","def load_mnist_labels(filename):\n","    # Read the labels in Yann LeCun's binary format.\n","    with gzip.open(filename, 'rb') as f:\n","        data = np.frombuffer(f.read(), np.uint8, offset=8)\n","    # The labels are vectors of integers now, that's exactly what we want.\n","    return data"],"metadata":{"id":"C46wOkga9r1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = load_image('data/train-images-idx3-ubyte.gz')\n","X_test = load_image('data/t10k-images-idx3-ubyte.gz')\n","Y_train = load_mnist_labels('data/train-labels-idx1-ubyte.gz')\n","Y_test = load_mnist_labels('data/t10k-labels-idx1-ubyte.gz')\n","# We reserve the last 10000 training examples for validation.\n","X_train, X_val = X_train[:-10000], X_train[-10000:]\n","Y_train, Y_val = Y_train[:-10000], Y_train[-10000:]"],"metadata":{"id":"82rvmn_69sPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('X_train: ' + str(X_train.shape))\n","print('Y_train: ' + str(Y_train.shape))\n","print('X_val: ' + str(X_val.shape))\n","print('Y_val: ' + str(Y_val.shape))\n","print('X_test:  '  + str(X_test.shape))\n","print('Y_test:  '  + str(Y_test.shape))"],"metadata":{"id":"17Iudfw-9tbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(331)\n","plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n","plt.show()\n","print()\n","print('Y_train[0]: ' + str(Y_train[0]))"],"metadata":{"id":"_AlqppAr9urZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing Data"],"metadata":{"id":"86NdHE5t-RMJ"}},{"cell_type":"markdown","source":["### Task 1:\n","\n","make one-hot encoding for labels. Clue: use [np.eye](https://numpy.org/doc/stable/reference/generated/numpy.eye.html) for them"],"metadata":{"id":"6Itazldc-ZsL"}},{"cell_type":"code","source":["def one_hot_encode(y):\n","    # YOUR CODE HERE:\n","    ###########################\n","    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","    ###########################\n","    one_hot_y = None\n","    return one_hot_y\n","\n","hot_y_train = one_hot_encode(Y_train)\n","hot_y_val = one_hot_encode(Y_val)\n","hot_y_test = one_hot_encode(Y_test)"],"metadata":{"id":"he8siulB-uZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test task 1"],"metadata":{"id":"USCt9saEsEmZ"}},{"cell_type":"code","source":["def one_hot_encode_test(hot_y_train):\n","    first_ten_answers = np.array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","                        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","                        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","                        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","                        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","                        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","                        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","                        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","                        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","                        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n","    np.testing.assert_equal(hot_y_train[:10], first_ten_answers, err_msg=\"First ten samples are not equal\")\n","    print(\"The test pass successfully !!!\")\n","\n","one_hot_encode_test(hot_y_train)"],"metadata":{"id":"kvmOLgIxsE2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 2:  \n","\n","In `homework_main-basic.ipynb` we treated mnist images as vectors, so we flattened it. For CNN, we assume that images have size `(bs, num_channels, w, h)`. Our mnist image is grayscale, so, it don't have a `num_channels` dimension. You need to reshape `X_train`, `X_val` and `X_test` to apropriate size."],"metadata":{"id":"Nc7RAuNF_MHx"}},{"cell_type":"code","source":["# YOUR CODE HERE:\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","X_train, X_val, X_test = None, None, None"],"metadata":{"id":"vWKipfHWAjHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test task 2"],"metadata":{"id":"dh-FJYZ6sH9o"}},{"cell_type":"code","source":["def dimension_test(X_train, X_val, X_test):\n","    true_train_shape = (50000, 1, 28, 28)\n","    true_test_shape = (10000, 1, 28, 28)\n","    np.testing.assert_equal(X_train.shape, true_train_shape, err_msg=\"Train shape doesn't the same\")\n","    np.testing.assert_equal(X_val.shape, true_test_shape, err_msg=\"Valid shape doesn't the same\")\n","    np.testing.assert_equal(X_test.shape, true_test_shape, err_msg=\"Test shape doesn't the same\")\n","    print(\"The test pass successfully !!!\")\n","\n","dimension_test(X_train, X_val, X_test)"],"metadata":{"id":"L-E893Z_sJTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN classification"],"metadata":{"id":"QUl0BCJIAs9r"}},{"cell_type":"markdown","source":["### Task 3:\n","\n","You need to define `in_features` for the final linear layer based on `kernel_size` and `out_channels` variables."],"metadata":{"id":"HozO1BIUsSq6"}},{"cell_type":"code","source":["def create_cnn(kernel_size, out_channels):\n","    CNN = Sequential()\n","    CNN.add(Conv2d(in_channels = 1, out_channels = out_channels, kernel_size = kernel_size))\n","    CNN.add(MaxPool2d(kernel_size = kernel_size))\n","    CNN.add(ReLU())\n","    CNN.add(Flatten())\n","\n","    # YOUR CODE HERE: Define `in_features` for variables `kernel_size`, `out_channels`\n","    ###########################\n","    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","    ###########################\n","    in_features = None\n","    CNN.add(Linear(in_features, 10))\n","    CNN.add(LogSoftMax())\n","    return CNN"],"metadata":{"id":"FQk4An8cB6gW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test task 3"],"metadata":{"id":"ixnhM157sbRF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLOLqF_guxQU"},"outputs":[],"source":["def test_cnn_creation():\n","    kernel_sizes = [1, 3, 5, 7]\n","    out_channels = [1, 2, 3, 4]\n","    rand_input = np.random.normal(size=(3, 1, 28, 28))\n","    try:\n","        rand_output = [create_cnn(ks, oc).forward(rand_input) for ks in kernel_sizes for oc in out_channels]\n","        print(\"The test pass successfully !!!\")\n","    except Exception as e:\n","        print(e.message)\n","        raise AssertionError\n","test_cnn_creation()"]},{"cell_type":"markdown","source":["### Task 4\n","\n","You need to fill gaps in the `train` pipeline. Note that `optimizer_name` can be one of `['sgd_momentum', 'adam_optimizer']`."],"metadata":{"id":"U-IWBZ6yskln"}},{"cell_type":"code","source":["# batch generator\n","def get_batches(dataset, batch_size):\n","    X, Y = dataset\n","    n_samples = X.shape[0]\n","\n","    # Shuffle at the start of epoch\n","    indices = np.arange(n_samples)\n","    np.random.shuffle(indices)\n","\n","    for start in range(0, n_samples, batch_size):\n","        end = min(start + batch_size, n_samples)\n","\n","        batch_idx = indices[start:end]\n","\n","        yield X[batch_idx], Y[batch_idx]"],"metadata":{"id":"7fr8n1_Ar_pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(net, criterion, optimizer_name, optimizer_config,\n","          n_epoch, X_train, y_train, X_val, y_val, batch_size):\n","\n","    loss_train_history = []\n","    loss_val_history = []\n","    optimizer_state = {}\n","\n","    for i in range(n_epoch):\n","        print('Epoch {}/{}:'.format(i, n_epoch - 1), flush=True)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                X = X_train\n","                y = y_train\n","                net.train()\n","            else:\n","                X = X_val\n","                y = y_val\n","                net.evaluate()\n","\n","            num_batches = X.shape[0] / batch_size\n","            running_loss = 0.\n","            running_acc = 0.\n","\n","            for x_batch, y_batch in get_batches((X, y), batch_size):\n","\n","                net.zeroGradParameters()\n","\n","                # Forward\n","                predictions = # Your code goes here\n","                loss = # Your code goes here\n","\n","                # Backward\n","                if phase == 'train':\n","                    dp = # Your code goes here\n","                    net.backward(x_batch, dp)\n","\n","                    # Update weights\n","                    if optimizer_name == 'sgd_momentum':\n","                        # Your code goes here\n","                    else:\n","                        # Your code goes here\n","\n","                running_loss += loss\n","                running_acc += np.sum(predictions.argmax(axis=1) == y_batch.argmax(axis=1))\n","\n","            epoch_loss = running_loss / num_batches\n","            epoch_acc = running_acc / y.shape[0]\n","            if phase == 'train':\n","                loss_train_history.append(epoch_loss)\n","            else:\n","                loss_val_history.append(epoch_loss)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n","\n","    return net, loss_train_history, loss_val_history"],"metadata":{"id":"0dCm1wl6sm4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 5\n","\n","You need to perform hyperparameter selection. \\\\\n","To narrow the search space, we fix `optimizer_name=sgd_momentum` and `batch_size=32`. So, there are neural network hyperparameters (`kernel_size` and `out_channels`) and optimizer's (`lr` and `momentum`). \\\\\n","See the structure of `sgd_momentum` to pass apropriate `optimizer_config` into `train` pipeline.\n","\n","Since the pipeline was not defined for sklearn, using [`RandomSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) may be inapropriate. Use [`ParameterSampler`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html) for them. The target criterion for selection is the validation loss.\n","\n","A neural network from scratch does not involve computations using GPUs, so the training time for one epoch is significant. In this regard, use 3 runs of hyperparameter selection on 3 epochs (**It will take about 50 minutes.**)."],"metadata":{"id":"vB_t4Y-xs6Og"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import ParameterSampler\n","\n","# YOUR CODE HERE: # Define the hyperparameter grid.\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","param_grid = None\n","\n","# YOUR CODE HERE: # Define a list of parameters. The length of list is should equal to 3\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","param_list = None\n","assert len(param_list) == 3, f\"We make a search only over 3 runs.\"\n","\n","best_loss = np.inf\n","best_params = None\n","results = []\n","n_epoch = 3\n","batch_size = 32\n","criterion = ClassNLLCriterion()\n","\n","for params in param_list:\n","    # YOUR CODE HERE:\n","    ###########################\n","    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","    ###########################\n","    # 1. Create network with the given hyperparameters.\n","    net = None\n","    # 2. Build the optimizer configuration for sgd_momentum.\n","    optimizer_config = None\n","\n","\n","    # Run training.\n","    net, loss_train_history, loss_val_history = train(\n","        net, criterion, 'sgd_momentum', optimizer_config,\n","        n_epoch, X_train, hot_y_train, X_val, hot_y_val, batch_size\n","    )\n","    # The final epoch's validation loss as the metric.\n","    final_val_loss = loss_val_history[-1]\n","    results.append((params, final_val_loss))\n","\n","    if final_val_loss < best_loss:\n","        best_loss = final_val_loss\n","        best_params = params\n","\n","print(\"Best hyperparameters:\", best_params)\n","print(\"Best validation loss: {:.4f}\".format(best_loss))"],"metadata":{"id":"ILB3TTGbs9To"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 6\n","\n","For the selected hyperparameters, you need to run 3-fold cross-validation. Use [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) for them (**It will also take about 50 minutes.**) \\\\\n","**Why is this necessary if we already make optimization over hyperparameter space?**"],"metadata":{"id":"N1byaA57tcy6"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","\n","# YOUR CODE HERE:\n","# Define a KFold Class for cross-validaton.\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","kf = None\n","\n","# YOUR CODE HERE:\n","# We need to merge images X_train and X_val\n","# and labels to make a random cross-validation\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","X_all = None\n","y_all = None\n","\n","best_nn_weights = None\n","best_cv_loss = np.inf\n","cv_losses = []\n","n_epoch = 3\n","batch_size = 32\n","criterion = ClassNLLCriterion()\n","\n","for fold, (train_index, val_index) in enumerate(kf.split(X_all)):\n","    # Split the full dataset into training and validation subsets for this fold.\n","    ###########################\n","    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","    ###########################\n","    X_train_cv, X_val_cv = None, None\n","    y_train_cv, y_val_cv = None, None\n","\n","    # Create a new instance of the network and optimizer using best hyperparameters.\n","    ###########################\n","    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","    ###########################\n","    net = None\n","    optimizer_config = None\n","\n","    # Train the model for this fold.\n","    net, loss_train_history, loss_val_history = train(\n","        net, criterion, 'sgd_momentum', optimizer_config,\n","        n_epoch, X_train_cv, y_train_cv, X_val_cv, y_val_cv, batch_size\n","    )\n","\n","    # Record the final validation loss for this fold.\n","    fold_val_loss = loss_val_history[-1]\n","\n","    # Save best final weights\n","    if fold_val_loss < best_cv_loss:\n","        best_cv_loss = fold_val_loss\n","        # getParameters of all modules in a sequential container.\n","        # clue: see the corresponding method in `Sequential` Module.\n","        ###########################\n","        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","        ###########################\n","        best_nn_weights = None"],"metadata":{"id":"Z9B8BM67tf--"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 7\n","\n","The final step is testing the model on a separate subset. Fill in the gaps in the `test` function. Initialize the neural network with the best hyperparameters and load the best weights."],"metadata":{"id":"6QxLJwXJt61v"}},{"cell_type":"code","source":["def test(net, criterion, X_test, y_test, batch_size):\n","    X_test, y_test = X_test[:3200], y_test[:3200]\n","    net.evaluate()\n","    num_batches = X_test.shape[0] / batch_size\n","    running_loss = 0.\n","    running_acc = 0.\n","    for x_batch, y_batch in get_batches((X_test, y_test), batch_size):\n","        net.zeroGradParameters()\n","\n","        # Forward\n","        predictions = # Your code goes here\n","        loss = # Your code goes here\n","        running_loss += loss\n","        running_acc += (predictions.argmax(axis=1) == y_batch.argmax(axis=1)).astype(float).mean()\n","\n","    epoch_loss = running_loss / num_batches\n","    epoch_acc = running_acc / num_batches\n","    print('Final Test Loss: {:.4f} Final Test Acc: {:.4f}'.format(epoch_loss, epoch_acc), flush=True)\n","\n","    return epoch_loss, epoch_acc\n","\n","\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################\n","# 1. Init CNN with `best_params`\n","net = None\n","# setParameters of CNN with `best_nn_weights`.\n","# clue: see the corresponding method in `Sequential` Module.\n","None\n","\n","epoch_loss, epoch_acc = test(net, criterion, X_test, hot_y_test, batch_size=32)"],"metadata":{"id":"42OI57P9t7OI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 8\n","\n","Now you need to apply your skills and creativity to improve the result:\n","1. Use more calculations (more detailed hyperparameter selection, more training epochs);\n","2. Create your own CNN with dropouts and batch normalization."],"metadata":{"id":"ZsDA5B8tuch2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"3VeAVEbUuxQV","colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"status":"ok","timestamp":1741604658207,"user_tz":-180,"elapsed":8,"user":{"displayName":"Сергей Скорик","userId":"16638388598197815420"}},"outputId":"ec61d813-8b43-498d-8698-cae42b03f973"},"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://memepedia.ru/wp-content/uploads/2017/08/%D0%B1%D0%B5%D0%BD%D0%B4%D0%B5%D1%80-%D1%84%D1%83%D1%82%D1%83%D1%80%D0%B0%D0%BC%D0%B0.png\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":1}],"source":["from IPython.display import Image\n","Image(url='https://memepedia.ru/wp-content/uploads/2017/08/%D0%B1%D0%B5%D0%BD%D0%B4%D0%B5%D1%80-%D1%84%D1%83%D1%82%D1%83%D1%80%D0%B0%D0%BC%D0%B0.png', width=700)"]},{"cell_type":"markdown","source":["3. Add augmentations to the training set.\n","4. Use adaptive optimization. Any other techniques are welcome!\n","\n","\n","At the output, you should get at least 97% on the test set."],"metadata":{"id":"rroV3KxYuhGc"}},{"cell_type":"code","source":["# YOUR CODE HERE:\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################"],"metadata":{"id":"AwPoRLsJujK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 9 Feel the power of the GPU!\n","\n","Let's achive the same results with [`PyTorch`](https://pytorch.org/). Use the framework syntax to create the [СNN, optimizer, training and testing loops](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). Don't forget to [convert](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) the data to the torch tensor format and wrap it with [Dataset and Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for correct processing."],"metadata":{"id":"nCLI7RB9ukW-"}},{"cell_type":"code","source":["# YOUR CODE HERE:\n","###########################\n","### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","###########################"],"metadata":{"id":"jpDIbt2KulbP"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}