{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiLaeZGnVKLW"
   },
   "source": [
    "# Part 1: Differentiation\n",
    "\n",
    "This task practice the basic calculation of scalar/vector/matrix differential by vector/matrix.\n",
    "\n",
    "In this task it is important to **learn** how to confidently take matrix derivatives (During the oral discussion the skill will be tested). We highly recommend looking at the definitions of [vector](https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors) and [matrix](https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices) derivatives.\n",
    "\n",
    "Use the notation from [YSDA](https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie). You can also use element-wise calculation of the derivative.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk6vzWRoVKLa"
   },
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhAXjgroVKLb"
   },
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxx2R8HWVKLb"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = 2x^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \\frac{d(x^Tx)}{dx} = \\frac{dx^T}{dx}x + x^T\\frac{dx}{dx} = E^Tx + x^TE = 2x^T \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHHytxvQVKLc"
   },
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRgSfEsQVKLc"
   },
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSxj3u04VKLc"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "tr(AB) =  \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} a_{ij} * b_{ji} \\\\\n",
    "\\frac{\\partial tr(AB)}{\\partial a_{pq}} = b_{qp} \\\\\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC8yj05nVKLd"
   },
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX8mEyIOVKLd"
   },
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEPdD6pgVKLe"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = Ac \\\\\n",
    "\n",
    "[D_{x_0}f](h) = f(x+h) - f(x)=(x+h)^TAc - x^TAc = x^TAc + h^TAc - x^TAc = h^TAc  \\\\\n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an4XTD3tVKLe"
   },
   "source": [
    "$$\n",
    "\\frac{dy}{dA} = xc^T \\\\\n",
    "[D_{A}f](H) = f(A+H) - f(A)=x^T(A+H)c - x^TAc = x^THс \\\\\n",
    "[D_{A}f](H) = tr((\\frac{dy}{dA})^TH) \\\\\n",
    "tr(x^THc) = tr(cx^TH) \\\\\n",
    "\\frac{dy}{dA} = xc^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM4AMStXVKLe"
   },
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact\n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SD1fJwJVKLf"
   },
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5S4BFC2VKLf"
   },
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgiwWgGGVKLf"
   },
   "source": [
    "### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T)\n",
    "$$\n",
    "it is easy to derive gradients (you can find it in one of the refs).\n",
    "\n",
    "$$\n",
    "J = tr((X-AS)(X-AS)^T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "JsKLbHpuVKLg"
   },
   "source": [
    "### Second approach\n",
    "We can use chain rule!\n",
    "let $ F = AS $\n",
    "\n",
    "$$\n",
    "J = tr((X-F)(X-F)^T) = tr((X-F)(X^T-F^T))=tr(XX^T - FX^T - XF^T + FF^T) = tr(XX^T)-tr(FX^T)-tr(XF^T)+tr(FF^T)\\\\\n",
    "\n",
    "tr(XF^T) = tr(FX^T)\\\\\n",
    "\n",
    "tr(FF^T) =  \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} f_{ij}^2 \\\\\n",
    "\n",
    "\\frac{dJ}{dF} = 0 - X - X + 2F = -2(X-F)\n",
    "$$\n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} = -2(X - F)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{dF}{dS} = A^T\\\\\n",
    "\n",
    "[D_SF](H) = A(S+H) - AS = AH \\\\\n",
    "tr(AH) = tr((\\frac{dF}{dS})^TH)\n",
    "$$\n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} = \\frac{dJ}{dF}\\frac{dF}{dS} = -2(X - AS)A^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**\n",
    "\n",
    "### Forward pass:\n",
    "$$\n",
    "    y_1 = x * w_1 + b_1\\\\\n",
    "    y_2 = y_1 * w_2 + b_2\n",
    "$$\n",
    "\n",
    "### Bacward pass:\n",
    "$$\n",
    "gradOutput_2 = \\frac{\\partial L}{\\partial y_2} \\\\\n",
    "\n",
    "gradInput_2 = \\frac{\\partial L}{\\partial y_1} = \\frac{\\partial L}{\\partial y_2} \\frac{\\partial y_2}{\\partial y_1} = gradOutput_2 * w_2\\\\\n",
    "\n",
    "gradOutput_1 = \\frac{\\partial L}{\\partial y_1} = gradInput_2 = gradOutput_2 * w_2 \\\\\n",
    "\n",
    "gradInput_1 = \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y_2} \\frac{\\partial y_2}{\\partial y_1} \\frac{\\partial y_1}{\\partial x} = gradOutput_1 * w_1\n",
    "$$\n",
    "\n",
    "### Update grad parameters:\n",
    "$$\n",
    "\n",
    "gradOutput = \\frac{\\partial L}{\\partial y}\\\\\n",
    "input = \\frac{\\partial y}{\\partial w} \\\\\n",
    "\n",
    "gradW = \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w} = gradOutput * input \\\\\n",
    "\n",
    "gradB = \\sum_{i = 1}^{batch\\_size} \\frac{\\partial L}{\\partial b_i} = \\sum_{i = 1}^{batch\\_size}\\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial b_i} = \\sum_{i = 1}^{batch\\_size} gradOutput[i:](?)\n",
    "$$\n",
    "\n",
    "## 2. SoftMax\n",
    "### Forward pass:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}\n",
    "$$\n",
    "\n",
    "### Backward pass:\n",
    "$$\n",
    "\n",
    "gradOutput = \\frac{dL}{dy}\\\\\n",
    "gradInput = \\frac{dL}{dx}\\\\\n",
    "\n",
    "gradInput = \\sum_{i = 1}^N\\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_k} \\\\\n",
    "$$\n",
    "\n",
    "Случай i == k:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_k} = \\frac{e^{x_i} \\sum_j e^{x_j} - e^{x_i} e^{x_k}}{(\\sum_j e^{x_j})^2} = y_i (1 - y_k)\n",
    "$$\n",
    "\n",
    "Случай i != k:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_k} = \\frac{- e^{x_i} e ^{x_k}}{(\\sum_j e^{x_j})^2} = - y_i \\cdot y_k\\\\\n",
    "\n",
    "\n",
    "gradInput = \\sum_{i = 1}^N \\frac{\\partial L}{\\partial y_i} y_i (1 - y_i) + \\sum_{i != k}^N \\frac{\\partial L}{\\partial y_i} (-y_i \\cdot y_k) \\\\\n",
    "gradInput = y (gradOutput - \\sum_{i = 1}^N gradOutput * y_i)\n",
    "$$\n",
    "\n",
    "## 3. Batch normalization\n",
    "\n",
    "### Forward pass:\n",
    "$$\n",
    "\\mu = \\frac{1}{N}\\sum_{i = 1}^{N}x_i \\\\\n",
    "\\sigma = \\frac{1}{N}\\sum_{i = 1}^{N}(x_i - \\mu)^2\\\\\n",
    "\\mu_{moving} = \\alpha \\cdot \\mu_{moving} + (1 - \\alpha) \\cdot \\mu \\\\\n",
    "\\sigma_{moving} = \\alpha \\cdot \\sigma_{moving} + (1 - \\alpha) \\cdot \\sigma \\\\\n",
    "\n",
    "output = \\frac{x - \\mu}{\\sqrt{\\sigma + \\epsilon}}\n",
    "$$\n",
    "### Backward pass:\n",
    "$$\n",
    "\n",
    "gradOutput = \\frac{dL}{dy}\\\\\n",
    "gradInput = \\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx} + \\frac{dL}{d\\mu}\\frac{d\\mu}{dx} + \\frac{dL}{d\\sigma}\\frac{d\\sigma}{dx}\\\\\n",
    "\n",
    "\\frac{dy}{dx} = \\frac{1}{\\sqrt{\\sigma + \\epsilon}}\\\\ \n",
    "\n",
    "\\frac{dL}{d\\mu} = \\sum_{i = 1}^{N} \\frac{dL}{dy} \\frac{dy}{d\\mu} = \\sum_{i = 1}^{N} gradOutput \\cdot \\frac{-1}{\\sqrt{\\sigma + \\epsilon}} \\\\\n",
    "\\frac{d\\mu}{dx} = \\frac{1}{N} \\\\\n",
    "\n",
    "\\frac{dL}{d\\sigma} = \\sum_{i = 1}^{N} \\frac{dL}{dy} \\frac{dy}{d\\sigma} = \\sum_{i = 1}^{N} gradOutput \\cdot \\frac{\\mu - x_i}{2 \\cdot (\\sigma + \\epsilon) ^ {3/2}} \\\\\n",
    "\\frac{d\\sigma}{dx} = \\frac{2}{N}\\sum_{i = 1}^{N} (x_i - \\mu)\\\\\n",
    "\n",
    "gradInput = gradOutput \\cdot \\frac{1}{\\sqrt{\\sigma + \\epsilon}} + \\frac{1}{N} \\cdot \\sum_{i = 1}^{N} gradOutput \\cdot \\frac{-1}{\\sqrt{\\sigma + \\epsilon}} + \\frac{x - \\mu}{N} \\cdot \\sum_{i = 1}^{N} gradOutput \\cdot \\frac{\\mu - x_i}{(\\sigma + \\epsilon) ^ {3/2}}\\\\\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
