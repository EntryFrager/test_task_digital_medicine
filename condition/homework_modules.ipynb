{"cells":[{"cell_type":"markdown","source":["# Implementation of neural network modules from scratch"],"metadata":{"id":"i8JWLW7roA5s"}},{"cell_type":"markdown","source":["## Task Description\n","\n","In this task, you need to implement the most common modules used to build deep neural networks. The modules have a fixed [torch](https://pytorch.org/)-like structure, so you only need to fill the forward and backward passes. All methods that require implementation are marked with a special comment `# Your code goes here. ################################################`.\n","\n","Here are the requirements for implementation:\n","\n","1. Each formula for updating gradients must be analytically justified (with `batch-size` first axis). For this, one can use the math. apparatus from `homework_differentiation.ipynb`.Element-wise differentiation is also welcome;\n","2. All code must be written only on [numpy](https://numpy.org/) unless otherwise specified (f.e. for Conv2d);\n","3. Each module has a set of tests. Successful implementation means successful passing of them.\n","\n","Good Luck!"],"metadata":{"id":"sY5mmCVGoziB"}},{"cell_type":"markdown","source":["## Module imports"],"metadata":{"id":"uPOnm2lvMf0q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYiY0-M0Anm1"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["## Testing imports"],"metadata":{"id":"m1Yiy0CYMirc"}},{"cell_type":"code","source":["import torch\n","from torch.autograd import Variable\n","from tqdm import tqdm"],"metadata":{"id":"YhZq3XXKMe4Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Basic Module"],"metadata":{"id":"6wDa8J3uu7Vu"}},{"cell_type":"markdown","metadata":{"id":"3lR5CCgVAnm4"},"source":["**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. **You do not need to change anything here, just read the comments.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rO8YhRMwAnm6"},"outputs":[],"source":["class Module(object):\n","    \"\"\"\n","    Basically, you can think of a module as of a something (black box)\n","    which can process `input` data and produce `ouput` data.\n","    This is like applying a function which is called `forward`:\n","\n","        output = module.forward(input)\n","\n","    The module should be able to perform a backward pass: to differentiate the `forward` function.\n","    More, it should be able to differentiate it if is a part of chain (chain rule).\n","    The latter implies there is a gradient from previous step of a chain rule.\n","\n","        gradInput = module.backward(input, gradOutput)\n","    \"\"\"\n","    def __init__ (self):\n","        self.output = None\n","        self.gradInput = None\n","        self.training = True\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Takes an input object, and computes the corresponding output of the module.\n","        \"\"\"\n","        return self.updateOutput(input)\n","\n","    def backward(self,input, gradOutput):\n","        \"\"\"\n","        Performs a backpropagation step through the module, with respect to the given input.\n","\n","        This includes\n","         - computing a gradient w.r.t. `input` (is needed for further backprop),\n","         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n","        \"\"\"\n","        self.updateGradInput(input, gradOutput)\n","        self.accGradParameters(input, gradOutput)\n","        return self.gradInput\n","\n","\n","    def updateOutput(self, input):\n","        \"\"\"\n","        Computes the output using the current parameter set of the class and input.\n","        This function returns the result which is stored in the `output` field.\n","\n","        Make sure to both store the data in `output` field and return it.\n","        \"\"\"\n","\n","        # The easiest case:\n","\n","        # self.output = input\n","        # return self.output\n","\n","        pass\n","\n","    def updateGradInput(self, input, gradOutput):\n","        \"\"\"\n","        Computing the gradient of the module with respect to its own input.\n","        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n","\n","        The shape of `gradInput` is always the same as the shape of `input`.\n","\n","        Make sure to both store the gradients in `gradInput` field and return it.\n","        \"\"\"\n","\n","        # The easiest case:\n","\n","        # self.gradInput = gradOutput\n","        # return self.gradInput\n","\n","        pass\n","\n","    def accGradParameters(self, input, gradOutput):\n","        \"\"\"\n","        Computing the gradient of the module with respect to its own parameters.\n","        No need to override if module has no parameters (e.g. ReLU).\n","        \"\"\"\n","        pass\n","\n","    def zeroGradParameters(self):\n","        \"\"\"\n","        Zeroes `gradParams` variable if the module has params.\n","        \"\"\"\n","        pass\n","\n","    def getParameters(self):\n","        \"\"\"\n","        Returns a list with its parameters.\n","        If the module does not have parameters return empty list.\n","        \"\"\"\n","        return []\n","\n","    def getGradParameters(self):\n","        \"\"\"\n","        Returns a list with gradients with respect to its parameters.\n","        If the module does not have parameters return empty list.\n","        \"\"\"\n","        return []\n","\n","    def setParameters(self, parameters):\n","        pass\n","\n","    def train(self):\n","        \"\"\"\n","        Sets training mode for the module.\n","        Training and testing behaviour differs for Dropout, BatchNorm.\n","        \"\"\"\n","        self.training = True\n","\n","    def evaluate(self):\n","        \"\"\"\n","        Sets evaluation mode for the module.\n","        Training and testing behaviour differs for Dropout, BatchNorm.\n","        \"\"\"\n","        self.training = False\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Pretty printing. Should be overrided in every module if you want\n","        to have readable description.\n","        \"\"\"\n","        return \"Module\""]},{"cell_type":"markdown","metadata":{"id":"BKkIIqkpAnm9"},"source":["# Layers"]},{"cell_type":"markdown","metadata":{"id":"523LtFbrAnm9"},"source":["## 1. Linear transform layer\n","Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n","- input:   **`batch_size x n_feats1`**\n","- output: **`batch_size x n_feats2`**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UcE4TtmAnm-"},"outputs":[],"source":["class Linear(Module):\n","    \"\"\"\n","    A module which applies a linear transformation\n","    A common name is fully-connected layer, InnerProductLayer in caffe.\n","\n","    The module should work with 2D input of shape (n_samples, n_feature).\n","    \"\"\"\n","    def __init__(self, n_in, n_out):\n","        super(Linear, self).__init__()\n","\n","        # This is a nice initialization\n","        stdv = 1./np.sqrt(n_in)\n","        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n","        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n","\n","        self.gradW = np.zeros_like(self.W)\n","        self.gradb = np.zeros_like(self.b)\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        # self.output = ...\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        # self.gradInput = ...\n","        return self.gradInput\n","\n","    def accGradParameters(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        # self.gradW = ... ; self.gradb = ...\n","        pass\n","\n","    def zeroGradParameters(self):\n","        self.gradW.fill(0)\n","        self.gradb.fill(0)\n","\n","    def getParameters(self):\n","        return [self.W, self.b]\n","\n","    def getGradParameters(self):\n","        return [self.gradW, self.gradb]\n","\n","    def setParameters(self, parameters):\n","        self.W = parameters[0]\n","        self.b = parameters[1]\n","\n","    def __repr__(self):\n","        s = self.W.shape\n","        q = 'Linear %d -> %d' %(s[1],s[0])\n","        return q"]},{"cell_type":"markdown","source":["### Test Linear"],"metadata":{"id":"zJE9ahG8MZHY"}},{"cell_type":"code","source":["def test_Linear():\n","    batch_size, n_in, n_out = 2, 3, 4\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","    for _ in tqdm(range(100), desc=\"Testing Linear layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.Linear(n_in, n_out)\n","        custom_layer = Linear(n_in, n_out)\n","        custom_layer.W = torch_layer.weight.data.numpy()\n","        custom_layer.b = torch_layer.bias.data.numpy()\n","\n","        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","        # 3. check layer parameters grad\n","        custom_layer.accGradParameters(layer_input, next_layer_grad)\n","        weight_grad = custom_layer.gradW\n","        bias_grad = custom_layer.gradb\n","        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","        np.testing.assert_allclose(\n","            torch_weight_grad, weight_grad, atol=3e-6,\n","            err_msg=\"Mismatch in weight gradients.\"\n","        )\n","        np.testing.assert_allclose(\n","            torch_bias_grad, bias_grad, atol=1e-6,\n","            err_msg=\"Mismatch in bias gradients.\"\n","        )\n","\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_Linear()"],"metadata":{"id":"LitZ6qQUMYkT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47ysyCLcAnm_"},"source":["## 2. SoftMax\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n","\n","Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ueRGb7ymAnm_"},"outputs":[],"source":["class SoftMax(Module):\n","    def __init__(self):\n","         super(SoftMax, self).__init__()\n","\n","    def updateOutput(self, input):\n","        # start with normalization for numerical stability\n","        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n","\n","        # Your code goes here. ################################################\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"SoftMax\""]},{"cell_type":"markdown","source":["### Test SoftMax"],"metadata":{"id":"oZRHDfgnmxiQ"}},{"cell_type":"code","source":["def test_SoftMax():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing SoftMax layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.Softmax(dim=1)\n","        custom_layer = SoftMax()\n","\n","        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","        next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","        next_layer_grad = next_layer_grad.clip(1e-5,1.)\n","        next_layer_grad = 1. / next_layer_grad\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-5,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1.5e-5,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_SoftMax()"],"metadata":{"id":"hTQEG2xrmy5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0IQ278WAnm_"},"source":["## 3. LogSoftMax\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n","\n","The main goal of this layer is to be used in computation of log-likelihood loss."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"YXP2dng5AnnA"},"outputs":[],"source":["class LogSoftMax(Module):\n","    def __init__(self):\n","         super(LogSoftMax, self).__init__()\n","\n","    def updateOutput(self, input):\n","        # start with normalization for numerical stability\n","        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n","\n","        # Your code goes here. ################################################\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"LogSoftMax\""]},{"cell_type":"markdown","source":["### Test LogSoftMax"],"metadata":{"id":"Opu0v33Um1gY"}},{"cell_type":"code","source":["def test_LogSoftMax():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing LogSoftMax layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.LogSoftmax(dim=1)\n","        custom_layer = LogSoftMax()\n","\n","        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","        next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_LogSoftMax()"],"metadata":{"id":"L-TaCEISm3gZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpjoUl4WAnnA"},"source":["## 4. Batch normalization\n","One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n","\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n","where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n","```\n","    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n","    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n","```\n","During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n","\n","Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6h2sUTzdAnnA"},"outputs":[],"source":["class BatchNormalization(Module):\n","    EPS = 1e-3\n","    def __init__(self, alpha = 0.):\n","        super(BatchNormalization, self).__init__()\n","        self.alpha = alpha\n","        self.moving_mean = None\n","        self.moving_variance = None\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        # use self.EPS please\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def getParameters(self):\n","        return [self.moving_mean, self.moving_variance]\n","\n","    def setParameters(self, parameters):\n","        self.moving_mean = parameters[0]\n","        self.moving_variance = parameters[1]\n","\n","    def __repr__(self):\n","        return \"BatchNormalization\""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VTq8QZvcAnnB"},"outputs":[],"source":["class ChannelwiseScaling(Module):\n","    \"\"\"\n","       Implements linear transform of input y = \\gamma * x + \\beta\n","       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n","    \"\"\"\n","    def __init__(self, n_out):\n","        super(ChannelwiseScaling, self).__init__()\n","\n","        stdv = 1./np.sqrt(n_out)\n","        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n","        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n","\n","        self.gradGamma = np.zeros_like(self.gamma)\n","        self.gradBeta = np.zeros_like(self.beta)\n","\n","    def updateOutput(self, input):\n","        self.output = input * self.gamma + self.beta\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = gradOutput * self.gamma\n","        return self.gradInput\n","\n","    def accGradParameters(self, input, gradOutput):\n","        self.gradBeta = np.sum(gradOutput, axis=0)\n","        self.gradGamma = np.sum(gradOutput*input, axis=0)\n","\n","    def zeroGradParameters(self):\n","        self.gradGamma.fill(0)\n","        self.gradBeta.fill(0)\n","\n","    def getParameters(self):\n","        return [self.gamma, self.beta]\n","\n","    def getGradParameters(self):\n","        return [self.gradGamma, self.gradBeta]\n","\n","    def setParameters(self, parameters):\n","        self.gamma = parameters[0]\n","        self.beta = parameters[1]\n","\n","    def __repr__(self):\n","        return \"ChannelwiseScaling\""]},{"cell_type":"markdown","metadata":{"id":"XwZTLLCmAnnB"},"source":["Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."]},{"cell_type":"markdown","source":["### Test BatchNorm"],"metadata":{"id":"gtNf-Ktim55j"}},{"cell_type":"code","source":["def test_BatchNormalization():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 32, 16\n","    for _ in tqdm(range(100), desc=\"Testing BacthNormalization layer\"):\n","        # layers initialization\n","        slope = np.random.uniform(0.01, 0.05)\n","        alpha = 0.9\n","        custom_layer = BatchNormalization(alpha)\n","        custom_layer.train()\n","        torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n","        custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","        custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n","        # with tolerance 1e-5\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-5,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","        # 3. check moving mean\n","        np.testing.assert_allclose(\n","            torch_layer.running_mean.numpy(),\n","            custom_layer.moving_mean,\n","            atol=1e-8,\n","            err_msg=\"Mismatch in moving mean between torch and custom layer.\"\n","        )\n","        # we don't check moving_variance because pytorch uses slightly different formula for it:\n","        # it computes moving average for unbiased variance (i.e var*N/(N-1))\n","        # np.testing.assert_allclose(\n","        #     torch_layer.running_var.numpy(),\n","        #     custom_layer.moving_variance,\n","        #     atol=1e-8,\n","        #     err_msg=\"Mismatch in moving variance between torch and custom layer.\"\n","        # )\n","\n","        # 4. check evaluation mode\n","        custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","        custom_layer.evaluate()\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        torch_layer.eval()\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer in evaluation mode.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_BatchNormalization()"],"metadata":{"id":"12L3yVmEm8KP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtKrsxsbAnnB"},"source":["## 5. Dropout\n","Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n","\n","This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n","\n","While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n","\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**"]},{"cell_type":"code","source":["class Dropout(Module):\n","    def __init__(self, p=0.5):\n","        super(Dropout, self).__init__()\n","\n","        self.p = p\n","        self.mask = None\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def getParameters(self):\n","        return [self.p, self.mask]\n","\n","    def setParameters(self, parameters):\n","        self.p = parameters[0]\n","        self.mask = parameters[1]\n","\n","    def __repr__(self):\n","        return \"Dropout\""],"metadata":{"id":"36b7fWJanGkE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test Dropout"],"metadata":{"id":"Vzrgc76SnDWg"}},{"cell_type":"code","source":["def test_Dropout():\n","    np.random.seed(42)\n","    batch_size, n_in = 2, 4\n","\n","    for _ in tqdm(range(100), desc=\"Testing Dropout layer\"):\n","        # layers initialization\n","        p = np.random.uniform(0.3, 0.7)\n","        layer = Dropout(p)\n","        layer.train()\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        layer_output = layer.updateOutput(layer_input)\n","        mask = np.logical_or(np.isclose(layer_output, 0),\n","                             np.isclose(layer_output * (1. - p), layer_input))\n","        np.testing.assert_array_equal(mask, True), \"Mismatch in dropout layer output.\"\n","\n","        # 2. check layer input grad\n","        layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","        mask_grad = np.logical_or(np.isclose(layer_grad, 0),\n","                                  np.isclose(layer_grad * (1. - p), next_layer_grad))\n","        np.testing.assert_array_equal(mask_grad, True), \"Mismatch in dropout layer grad input.\"\n","\n","        # 3. check evaluation mode\n","        layer.evaluate()\n","        layer_output = layer.updateOutput(layer_input)\n","        np.testing.assert_allclose(layer_output, layer_input, atol=1e-6), \"Dropout evaluation mode failed.\"\n","\n","\n","        # 4. check mask\n","        p = 0.0\n","        layer = Dropout(p)\n","        layer.train()\n","        layer_output = layer.updateOutput(layer_input)\n","        np.testing.assert_allclose(layer_output, layer_input, atol=1e-6), \"Dropout with p=0 should return input unchanged.\"\n","\n","        p = 0.5\n","        layer = Dropout(p)\n","        layer.train()\n","        layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","        layer_output = layer.updateOutput(layer_input)\n","        zeroed_elem_mask = np.isclose(layer_output, 0)\n","        layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","        np.testing.assert_array_equal(zeroed_elem_mask, np.isclose(layer_grad, 0)), \"Mismatch in dropout mask.\"\n","\n","        # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n","        batch_size_large, n_in_large = 1000, 1\n","        p = 0.8\n","        layer = Dropout(p)\n","        layer.train()\n","\n","        layer_input = np.random.uniform(5, 10, (batch_size_large, n_in_large)).astype(np.float32)\n","        layer_output = layer.updateOutput(layer_input)\n","        assert np.sum(np.isclose(layer_output, 0)) != layer_input.size, \"Dropout mask should be applied independently.\"\n","\n","        # Test with transposed input\n","        layer_input = layer_input.T\n","        layer_output = layer.updateOutput(layer_input)\n","        assert np.sum(np.isclose(layer_output, 0)) != layer_input.size, \"Dropout mask should be applied independently for transposed input.\"\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_Dropout()"],"metadata":{"id":"zcScB4iXnDzA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sequential container"],"metadata":{"id":"5trEjmwbBjJh"}},{"cell_type":"markdown","source":["**Define** a forward and backward pass procedures."],"metadata":{"id":"KnSJ2eYdBpX2"}},{"cell_type":"code","source":["class Sequential(Module):\n","    \"\"\"\n","         This class implements a container, which processes `input` data sequentially.\n","\n","         `input` is processed by each module (layer) in self.modules consecutively.\n","         The resulting array is called `output`.\n","    \"\"\"\n","\n","    def __init__ (self):\n","        super(Sequential, self).__init__()\n","        self.modules = []\n","\n","    def add(self, module):\n","        \"\"\"\n","        Adds a module to the container.\n","        \"\"\"\n","        self.modules.append(module)\n","\n","    def updateOutput(self, input):\n","        \"\"\"\n","        Basic workflow of FORWARD PASS:\n","\n","            y_0    = module[0].forward(input)\n","            y_1    = module[1].forward(y_0)\n","            ...\n","            output = module[n-1].forward(y_{n-2})\n","\n","\n","        Just write a little loop.\n","        \"\"\"\n","\n","        # Your code goes here. ################################################\n","        return self.output\n","\n","    def backward(self, input, gradOutput):\n","        \"\"\"\n","        Workflow of BACKWARD PASS:\n","\n","            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n","            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n","            ...\n","            g_1 = module[1].backward(y_0, g_2)\n","            gradInput = module[0].backward(input, g_1)\n","\n","\n","        !!!\n","\n","        To ech module you need to provide the input, module saw while forward pass,\n","        it is used while computing gradients.\n","        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n","        and NOT `input` to this Sequential module.\n","\n","        !!!\n","\n","        \"\"\"\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","\n","    def zeroGradParameters(self):\n","        for module in self.modules:\n","            module.zeroGradParameters()\n","\n","    def getGradParameters(self):\n","        \"\"\"\n","        Should gather all gradients w.r.t parameters in a list.\n","        \"\"\"\n","        return [x.getGradParameters() for x in self.modules]\n","\n","    def setParameters(self, parameters):\n","        for x, parameter in zip(self.modules, parameters):\n","            x.setParameters(parameter)\n","\n","    def getGradParameters(self):\n","        \"\"\"\n","        Should gather all gradients w.r.t parameters in a list.\n","        \"\"\"\n","        return [x.getGradParameters() for x in self.modules]\n","\n","    def __repr__(self):\n","        string = \"\".join([str(x) + '\\n' for x in self.modules])\n","        return string\n","\n","    def __getitem__(self,x):\n","        return self.modules.__getitem__(x)\n","\n","    def train(self):\n","        \"\"\"\n","        Propagates training parameter through all modules\n","        \"\"\"\n","        self.training = True\n","        for module in self.modules:\n","            module.train()\n","\n","    def evaluate(self):\n","        \"\"\"\n","        Propagates training parameter through all modules\n","        \"\"\"\n","        self.training = False\n","        for module in self.modules:\n","            module.evaluate()"],"metadata":{"id":"MHxbLWnUBi_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test Sequential"],"metadata":{"id":"FWYPwkG7nMyx"}},{"cell_type":"code","source":["def test_Sequential():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing Sequential layer\"):\n","        # layers initialization\n","        alpha = 0.9\n","        torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n","        torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n","        custom_layer = Sequential()\n","        bn_layer = BatchNormalization(alpha)\n","        bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","        bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","        custom_layer.add(bn_layer)\n","        scaling_layer = ChannelwiseScaling(n_in)\n","        scaling_layer.gamma = torch_layer.weight.data.numpy()\n","        scaling_layer.beta = torch_layer.bias.data.numpy()\n","        custom_layer.add(scaling_layer)\n","        custom_layer.train()\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=4e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=2e-4,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","        # 3. check layer parameters grad\n","        weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n","        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","        np.testing.assert_allclose(\n","            torch_weight_grad, weight_grad, atol=2e-6,\n","            err_msg=\"Mismatch in weight gradients.\"\n","        )\n","        np.testing.assert_allclose(\n","            torch_bias_grad, bias_grad, atol=1e-6,\n","            err_msg=\"Mismatch in bias gradients.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_Sequential()"],"metadata":{"id":"d_yrdzOYnORC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6jRQP95AnnC"},"source":["# Activation functions"]},{"cell_type":"markdown","metadata":{"id":"WfTj_TpSAnnC"},"source":["Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mC5FJ7GJAnnC"},"outputs":[],"source":["class ReLU(Module):\n","    def __init__(self):\n","         super(ReLU, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = np.maximum(input, 0)\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = np.multiply(gradOutput , input > 0)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ReLU\""]},{"cell_type":"markdown","metadata":{"id":"ksfMS461AnnC"},"source":["## 6. Leaky ReLU\n","Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYbdBv06AnnC"},"outputs":[],"source":["class LeakyReLU(Module):\n","    def __init__(self, slope = 0.03):\n","        super(LeakyReLU, self).__init__()\n","\n","        self.slope = slope\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"LeakyReLU\""]},{"cell_type":"markdown","source":["### Test Leaky ReLU"],"metadata":{"id":"Bzxj4oBGnVys"}},{"cell_type":"code","source":["def test_LeakyReLU():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing LeakyReLU layer\"):\n","        # layers initialization\n","        slope = np.random.uniform(0.01, 0.05)\n","        torch_layer = torch.nn.LeakyReLU(slope)\n","        custom_layer = LeakyReLU(slope)\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_LeakyReLU()"],"metadata":{"id":"En_3MPb1nWEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sx0cyHJuAnnD"},"source":["## 7. ELU\n","Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"q5muHa9ZAnnD"},"outputs":[],"source":["class ELU(Module):\n","    def __init__(self, alpha = 1.0):\n","        super(ELU, self).__init__()\n","\n","        self.alpha = alpha\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ELU\""]},{"cell_type":"markdown","source":["### Test ELU"],"metadata":{"id":"eBEcajNcnXle"}},{"cell_type":"code","source":["def test_ELU():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing ELU layer\"):\n","        # layers initialization\n","        alpha = 1.0\n","        torch_layer = torch.nn.ELU(alpha)\n","        custom_layer = ELU(alpha)\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_ELU()"],"metadata":{"id":"cFtsW8BhnY5v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZC8MBcdTAnnD"},"source":["## 8. SoftPlus\n","Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Myay1OqyAnnD"},"outputs":[],"source":["class SoftPlus(Module):\n","    def __init__(self):\n","        super(SoftPlus, self).__init__()\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"SoftPlus\""]},{"cell_type":"markdown","source":["### Test SoftPlus"],"metadata":{"id":"WxT66CgHnbrm"}},{"cell_type":"code","source":["def test_SoftPlus():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing SoftPlus layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.Softplus()\n","        custom_layer = SoftPlus()\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_SoftPlus()"],"metadata":{"id":"B6WhbnTwnc4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6L3qGpkyAnnD"},"source":["# Criterions"]},{"cell_type":"markdown","metadata":{"id":"9t-A5CxNAnnE"},"source":["Criterions are used to score the models answers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WYS7p7fAnnE"},"outputs":[],"source":["class Criterion(object):\n","    def __init__ (self):\n","        self.output = None\n","        self.gradInput = None\n","\n","    def forward(self, input, target):\n","        \"\"\"\n","            Given an input and a target, compute the loss function\n","            associated to the criterion and return the result.\n","\n","            For consistency this function should not be overrided,\n","            all the code goes in `updateOutput`.\n","        \"\"\"\n","        return self.updateOutput(input, target)\n","\n","    def backward(self, input, target):\n","        \"\"\"\n","            Given an input and a target, compute the gradients of the loss function\n","            associated to the criterion and return the result.\n","\n","            For consistency this function should not be overrided,\n","            all the code goes in `updateGradInput`.\n","        \"\"\"\n","        return self.updateGradInput(input, target)\n","\n","    def updateOutput(self, input, target):\n","        \"\"\"\n","        Function to override.\n","        \"\"\"\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        \"\"\"\n","        Function to override.\n","        \"\"\"\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Pretty printing. Should be overrided in every module if you want\n","        to have readable description.\n","        \"\"\"\n","        return \"Criterion\""]},{"cell_type":"markdown","metadata":{"id":"jdBoXF18AnnE"},"source":["The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n","- input:   **`batch_size x n_feats`**\n","- target: **`batch_size x n_feats`**\n","- output: **scalar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-TuMBaLAnnE"},"outputs":[],"source":["class MSECriterion(Criterion):\n","    def __init__(self):\n","        super(MSECriterion, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        self.gradInput  = (input - target) * 2 / input.shape[0]\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"MSECriterion\""]},{"cell_type":"markdown","metadata":{"id":"Tt9xBtqmAnnF"},"source":["## 9. Negative LogLikelihood criterion (numerically unstable)\n","You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n","remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n","- input:   **`batch_size x n_feats`** - probabilities\n","- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n","- output: **scalar**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_BTup6SAnnF"},"outputs":[],"source":["class ClassNLLCriterionUnstable(Criterion):\n","    EPS = 1e-15\n","    def __init__(self):\n","        a = super(ClassNLLCriterionUnstable, self)\n","        super(ClassNLLCriterionUnstable, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","\n","        # Use this trick to avoid numerical errors\n","        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n","\n","        # Your code goes here. ################################################\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","\n","        # Use this trick to avoid numerical errors\n","        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n","\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ClassNLLCriterionUnstable\""]},{"cell_type":"markdown","source":["### Test NLL (Unstable)"],"metadata":{"id":"843v3HuwnhpD"}},{"cell_type":"code","source":["def test_NLLCriterionUnstable():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing NLLCriterionUnstable layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.NLLLoss()\n","        custom_layer = ClassNLLCriterionUnstable()\n","\n","        layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n","        layer_input /= layer_input.sum(axis=-1, keepdims=True)\n","        layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n","        target_labels = np.random.choice(n_in, batch_size)\n","        target = np.zeros((batch_size, n_in), np.float32)\n","        target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n","                                                Variable(torch.from_numpy(target_labels), requires_grad=False))\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","        torch_layer_output_var.backward()\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_NLLCriterionUnstable()"],"metadata":{"id":"gMA46o76nh62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXPiigNIAnnF"},"source":["## 10. Negative LogLikelihood criterion (numerically stable)\n","- input:   **`batch_size x n_feats`** - log probabilities\n","- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n","- output: **scalar**\n","\n","Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8h_0CgokAnnF"},"outputs":[],"source":["class ClassNLLCriterion(Criterion):\n","    def __init__(self):\n","        a = super(ClassNLLCriterion, self)\n","        super(ClassNLLCriterion, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","        # Your code goes here. ################################################\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        # Your code goes here. ################################################\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ClassNLLCriterion\""]},{"cell_type":"markdown","source":["### Test NLL (Stable)"],"metadata":{"id":"R8G2dRB9njj0"}},{"cell_type":"code","source":["def test_NLLCriterion():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 4\n","    for _ in tqdm(range(100), desc=\"Testing NLLCriterion layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.NLLLoss()\n","        custom_layer = ClassNLLCriterion()\n","\n","        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","        layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n","        target_labels = np.random.choice(n_in, batch_size)\n","        target = np.zeros((batch_size, n_in), np.float32)\n","        target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var,\n","                                                Variable(torch.from_numpy(target_labels), requires_grad=False))\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","        torch_layer_output_var.backward()\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_NLLCriterion()"],"metadata":{"id":"ihJrX9flnk13"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFu8GaU6AnnF"},"source":["# Optimizers"]},{"cell_type":"markdown","metadata":{"id":"pb1I5vO4AnnG"},"source":["### SGD optimizer with momentum\n","- `variables` - list of lists of variables (one list per layer)\n","- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n","- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n","- `state` - dict with optimizator state (used to save accumulated gradients)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTtVqznmAnnN"},"outputs":[],"source":["def sgd_momentum(variables, gradients, config, state):\n","    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n","    state.setdefault('accumulated_grads', {})\n","\n","    var_index = 0\n","    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n","        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n","\n","            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n","\n","            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n","\n","            current_var -= old_grad\n","            var_index += 1"]},{"cell_type":"markdown","metadata":{"id":"7uiPD5AKAnnO"},"source":["## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n","- `variables` - list of lists of variables (one list per layer)\n","- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n","- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n","- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n","\n","Formulas for optimizer:\n","\n","Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n","First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$\n","Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n","New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLrob5JrAnnO"},"outputs":[],"source":["def adam_optimizer(variables, gradients, config, state):\n","    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n","    state.setdefault('m', {})  # first moment vars\n","    state.setdefault('v', {})  # second moment vars\n","    state.setdefault('t', 0)   # timestamp\n","    state['t'] += 1\n","    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n","        assert k in config, config.keys()\n","\n","    var_index = 0\n","    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n","    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n","        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n","            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n","            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n","\n","            # <YOUR CODE> #######################################\n","            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n","            #np.add(... , out=var_first_moment)\n","            #np.add(... , out=var_second_moment)\n","            #current_var -= ...\n","\n","            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n","            assert var_first_moment is state['m'].get(var_index)\n","            assert var_second_moment is state['v'].get(var_index)\n","            var_index += 1\n"]},{"cell_type":"markdown","source":["### Test Adam Optimizer"],"metadata":{"id":"Hir6MbWkno-y"}},{"cell_type":"code","source":["def test_adam_optimizer():\n","    state = {}\n","    config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}\n","    variables = [[np.arange(10).astype(np.float64)]]\n","    gradients = [[np.arange(10).astype(np.float64)]]\n","\n","    # First update\n","    adam_optimizer(variables, gradients, config, state)\n","    np.testing.assert_allclose(\n","        state['m'][0],\n","        np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n","        err_msg=\"Mismatch in state['m'] after first update\"\n","    )\n","    np.testing.assert_allclose(\n","        state['v'][0],\n","        np.array([0.0, 0.001, 0.004, 0.009, 0.016, 0.025, 0.036, 0.049, 0.064, 0.081]),\n","        err_msg=\"Mismatch in state['v'] after first update\"\n","    )\n","    np.testing.assert_equal(\n","        state['t'],\n","        1,\n","        err_msg=\"Mismatch in state['t'] after first update\"\n","    )\n","    np.testing.assert_allclose(\n","        variables[0][0],\n","        np.array([0.0, 0.999, 1.999, 2.999, 3.999, 4.999, 5.999, 6.999, 7.999, 8.999]),\n","        err_msg=\"Mismatch in variables after first update\"\n","    )\n","\n","    # Second update\n","    adam_optimizer(variables, gradients, config, state)\n","    np.testing.assert_allclose(\n","        state['m'][0],\n","        np.array([0.0, 0.19, 0.38, 0.57, 0.76, 0.95, 1.14, 1.33, 1.52, 1.71]),\n","        err_msg=\"Mismatch in state['m'] after second update\"\n","    )\n","    np.testing.assert_allclose(\n","        state['v'][0],\n","        np.array([0.0, 0.001999, 0.007996, 0.017991, 0.031984, 0.049975, 0.071964, 0.097951, 0.127936, 0.161919]),\n","        err_msg=\"Mismatch in state['v'] after second update\"\n","    )\n","    np.testing.assert_equal(\n","        state['t'],\n","        2,\n","        err_msg=\"Mismatch in state['t'] after second update\"\n","    )\n","    np.testing.assert_allclose(\n","        variables[0][0],\n","        np.array([0.0, 0.998, 1.998, 2.998, 3.998, 4.998, 5.998, 6.998, 7.998, 8.998]),\n","        err_msg=\"Mismatch in variables after second update\"\n","    )\n","\n","    print(\"All tests passed successfully!\")\n","\n","test_adam_optimizer()"],"metadata":{"id":"To-R37yLnpTd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WeVqfTX1AnnO"},"source":["# Layers for advanced track homework\n","You **don't need** to implement it if you are working on `homework_main-basic.ipynb`"]},{"cell_type":"markdown","metadata":{"id":"sMQG4Gd7AnnP"},"source":["## 12. Conv2d [Advanced]\n","- input:   **`batch_size x in_channels x h x w`**\n","- output: **`batch_size x out_channels x h x w`**\n","\n","You should implement something like pytorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n","\n","Practical notes:\n","- While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n","- It may be convenient to use `np.pad` for zero-padding.\n","- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBSIFb9GAnnP"},"outputs":[],"source":["import scipy as sp\n","import scipy.signal\n","\n","class Conv2d(Module):\n","    def __init__(self, in_channels, out_channels, kernel_size):\n","        super(Conv2d, self).__init__()\n","        assert kernel_size % 2 == 1, kernel_size\n","\n","        stdv = 1./np.sqrt(in_channels)\n","        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n","        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","\n","        self.gradW = np.zeros_like(self.W)\n","        self.gradb = np.zeros_like(self.b)\n","\n","    def updateOutput(self, input):\n","        pad_size = self.kernel_size // 2\n","        # YOUR CODE ##############################\n","        # 1. zero-pad the input array\n","        # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n","        # 3. add bias value\n","\n","        # self.output = ...\n","\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        pad_size = self.kernel_size // 2\n","        # YOUR CODE ##############################\n","        # 1. zero-pad the gradOutput\n","        # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n","\n","        # self.gradInput = ...\n","\n","        return self.gradInput\n","\n","    def accGradParameters(self, input, gradOutput):\n","        pad_size = self.kernel_size // 2\n","        # YOUR CODE #############\n","        # 1. zero-pad the input\n","        # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n","        # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n","\n","        # self.gradW = ...\n","        # self.gradb = ...\n","        pass\n","\n","    def zeroGradParameters(self):\n","        self.gradW.fill(0)\n","        self.gradb.fill(0)\n","\n","    def getParameters(self):\n","        return [self.W, self.b]\n","\n","    def getGradParameters(self):\n","        return [self.gradW, self.gradb]\n","\n","    def setParameters(self, parameters):\n","        self.W = parameters[0]\n","        self.b = parameters[1]\n","\n","    def __repr__(self):\n","        s = self.W.shape\n","        q = 'Conv2d %d -> %d' %(s[1],s[0])\n","        return q"]},{"cell_type":"markdown","source":["### Test Conv2d"],"metadata":{"id":"9Er6t9Lxnt1S"}},{"cell_type":"code","source":["def test_Conv2d():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in, n_out = 2, 3, 4\n","    h,w = 5,6\n","    kern_size = 3\n","    for _ in tqdm(range(100), desc=\"Testing Conv2d layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.Conv2d(n_in, n_out, kern_size, padding=1)\n","        custom_layer = Conv2d(n_in, n_out, kern_size)\n","        custom_layer.W = torch_layer.weight.data.numpy() # [n_out, n_in, kern, kern]\n","        custom_layer.b = torch_layer.bias.data.numpy()\n","\n","        layer_input = np.random.uniform(-1, 1, (batch_size, n_in, h,w)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_out, h, w)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","        # 3. check layer parameters grad\n","        custom_layer.accGradParameters(layer_input, next_layer_grad)\n","        weight_grad = custom_layer.gradW\n","        bias_grad = custom_layer.gradb\n","        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","        #m = ~np.isclose(torch_weight_grad, weight_grad, atol=1e-5)\n","        np.testing.assert_allclose(\n","            torch_weight_grad, weight_grad, atol=4e-6,\n","            err_msg=\"Mismatch in weight gradients.\"\n","        )\n","        np.testing.assert_allclose(\n","            torch_bias_grad, bias_grad, atol=4e-6,\n","            err_msg=\"Mismatch in bias gradients.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_Conv2d()"],"metadata":{"id":"Q06mUrLGntj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLWWvZX2AnnP"},"source":["## 13. MaxPool2d [Advanced]\n","- input:   **`batch_size x n_input_channels x h x w`**\n","- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n","\n","You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n","\n","Practical notes:\n","- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n","- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n","- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHSenFIHAnnQ"},"outputs":[],"source":["class MaxPool2d(Module):\n","    def __init__(self, kernel_size):\n","        super(MaxPool2d, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.gradInput = None\n","\n","    def updateOutput(self, input):\n","        input_h, input_w = input.shape[-2:]\n","        # your may remove these asserts and implement MaxPool2d with padding\n","        assert input_h % self.kernel_size == 0\n","        assert input_w % self.kernel_size == 0\n","\n","        # YOUR CODE #############################\n","        # self.output = ...\n","        # self.max_indices = ...\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # YOUR CODE #############################\n","        # self.gradInput = ...\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n","        return q"]},{"cell_type":"markdown","source":["### Test MaxPool2d"],"metadata":{"id":"cTTH7nFTn0eJ"}},{"cell_type":"code","source":["def test_MaxPool2d():\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","\n","    batch_size, n_in = 2, 3\n","    h,w = 4,6\n","    kern_size = 2\n","    for _ in tqdm(range(100), desc=\"Testing MaxPool2d layer\"):\n","        # layers initialization\n","        torch_layer = torch.nn.MaxPool2d(kern_size)\n","        custom_layer = MaxPool2d(kern_size)\n","\n","        layer_input = np.random.uniform(-10, 10, (batch_size, n_in, h,w)).astype(np.float32)\n","        next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_in,\n","                                                        h // kern_size, w // kern_size)).astype(np.float32)\n","\n","        # 1. check layer output\n","        custom_layer_output = custom_layer.updateOutput(layer_input)\n","        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","        torch_layer_output_var = torch_layer(layer_input_var)\n","        np.testing.assert_allclose(\n","            torch_layer_output_var.data.numpy(),\n","            custom_layer_output,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in forward output between torch and custom layer.\"\n","        )\n","\n","        # 2. check layer input grad\n","        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","        torch_layer_grad_var = layer_input_var.grad\n","        np.testing.assert_allclose(\n","            torch_layer_grad_var.data.numpy(),\n","            custom_layer_grad,\n","            atol=1e-6,\n","            err_msg=\"Mismatch in gradient wrt input between torch and custom layer.\"\n","        )\n","\n","    print(\"\\nAll tests passed successfully!\")\n","\n","test_MaxPool2d()"],"metadata":{"id":"eJdwP4hQn2Dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFOHkdHfAnnQ"},"source":["### Flatten layer\n","Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrUYrwlpAnnQ"},"outputs":[],"source":["class Flatten(Module):\n","    def __init__(self):\n","         super(Flatten, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = input.reshape(len(input), -1)\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = gradOutput.reshape(input.shape)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"Flatten\""]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}